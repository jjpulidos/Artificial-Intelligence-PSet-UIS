{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecciona un dataset del [repositorio de la UCI](http://archive.ics.uci.edu/ml/), define una tarea de clasificación binaria y realiza el mismo proceso que en las Notas correspondientes.\n",
    "\n",
    "### Prueba varias estructuras de redes y compara el desempeño en TEST con alguno de los métodos usados en las Notas 02B "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(699, 11)\n",
      "(699, 10) [[  1.00002500e+06   5.00000000e+00   1.00000000e+00 ...,   3.00000000e+00\n",
      "    1.00000000e+00   1.00000000e+00]\n",
      " [  1.00294500e+06   5.00000000e+00   4.00000000e+00 ...,   3.00000000e+00\n",
      "    2.00000000e+00   1.00000000e+00]\n",
      " [  1.01542500e+06   3.00000000e+00   1.00000000e+00 ...,   3.00000000e+00\n",
      "    1.00000000e+00   1.00000000e+00]\n",
      " ..., \n",
      " [  8.88820000e+05   5.00000000e+00   1.00000000e+01 ...,   8.00000000e+00\n",
      "    1.00000000e+01   2.00000000e+00]\n",
      " [  8.97471000e+05   4.00000000e+00   8.00000000e+00 ...,   1.00000000e+01\n",
      "    6.00000000e+00   1.00000000e+00]\n",
      " [  8.97471000e+05   4.00000000e+00   8.00000000e+00 ...,   1.00000000e+01\n",
      "    4.00000000e+00   1.00000000e+00]]\n",
      "(699,) [ 2.  2.  2.  2.  2.  4.  2.  2.  2.  2.  2.  2.  4.  2.  4.  4.  2.  2.\n",
      "  4.  2.  4.  4.  2.  4.  2.  4.  2.  2.  2.  2.  2.  2.  4.  2.  2.  2.\n",
      "  4.  2.  4.  4.  2.  4.  4.  4.  4.  2.  4.  2.  2.  4.  4.  4.  4.  4.\n",
      "  4.  4.  4.  4.  4.  4.  4.  2.  4.  4.  2.  4.  2.  4.  4.  2.  2.  4.\n",
      "  2.  4.  4.  2.  2.  2.  2.  2.  2.  2.  2.  2.  4.  4.  4.  4.  2.  2.\n",
      "  2.  2.  2.  2.  2.  2.  2.  2.  4.  4.  4.  4.  2.  4.  4.  4.  4.  4.\n",
      "  2.  4.  2.  4.  4.  4.  2.  2.  2.  4.  2.  2.  2.  2.  4.  4.  4.  2.\n",
      "  4.  2.  4.  2.  2.  2.  4.  2.  2.  2.  2.  2.  2.  2.  2.  2.  4.  2.\n",
      "  2.  2.  4.  2.  2.  4.  2.  4.  4.  2.  2.  4.  2.  2.  2.  4.  4.  2.\n",
      "  2.  2.  2.  2.  4.  4.  2.  2.  2.  2.  2.  4.  4.  4.  2.  4.  2.  4.\n",
      "  2.  2.  2.  4.  4.  2.  4.  4.  4.  2.  4.  4.  2.  2.  2.  2.  2.  2.\n",
      "  2.  2.  4.  4.  2.  2.  2.  4.  4.  2.  2.  2.  4.  4.  2.  4.  4.  4.\n",
      "  2.  2.  4.  2.  2.  4.  4.  4.  4.  2.  4.  4.  2.  4.  4.  4.  2.  4.\n",
      "  2.  2.  4.  4.  4.  4.  2.  2.  2.  2.  2.  2.  4.  4.  2.  2.  2.  4.\n",
      "  2.  4.  4.  4.  2.  2.  2.  2.  4.  4.  4.  4.  4.  2.  4.  4.  4.  2.\n",
      "  4.  2.  4.  4.  2.  2.  2.  2.  2.  4.  2.  2.  4.  4.  4.  4.  4.  2.\n",
      "  4.  4.  2.  2.  4.  4.  2.  4.  2.  2.  2.  4.  4.  2.  4.  2.  4.  4.\n",
      "  2.  2.  4.  2.  2.  2.  4.  2.  2.  2.  4.  4.  2.  2.  4.  2.  2.  4.\n",
      "  2.  2.  4.  2.  4.  4.  4.  2.  2.  4.  4.  2.  4.  2.  2.  4.  4.  2.\n",
      "  2.  2.  4.  2.  2.  2.  4.  4.  2.  2.  2.  4.  2.  2.  4.  4.  4.  4.\n",
      "  4.  4.  2.  2.  2.  2.  4.  4.  2.  2.  2.  2.  2.  2.  2.  2.  2.  2.\n",
      "  2.  2.  2.  4.  2.  2.  2.  2.  4.  2.  2.  2.  2.  4.  2.  2.  2.  2.\n",
      "  2.  2.  2.  2.  4.  2.  2.  2.  2.  2.  2.  2.  2.  2.  2.  2.  4.  2.\n",
      "  4.  2.  4.  2.  2.  2.  2.  4.  2.  2.  2.  4.  2.  4.  2.  2.  2.  2.\n",
      "  2.  2.  2.  4.  4.  2.  2.  2.  4.  2.  2.  2.  2.  2.  2.  2.  2.  4.\n",
      "  2.  2.  2.  4.  2.  4.  4.  4.  2.  2.  2.  2.  2.  2.  2.  4.  4.  4.\n",
      "  2.  2.  2.  2.  2.  2.  2.  2.  2.  2.  2.  4.  2.  2.  4.  4.  2.  2.\n",
      "  2.  4.  4.  4.  2.  4.  2.  4.  2.  2.  2.  2.  2.  2.  2.  2.  2.  2.\n",
      "  2.  2.  4.  2.  2.  2.  2.  2.  2.  2.  4.  4.  2.  2.  2.  4.  2.  2.\n",
      "  4.  4.  2.  2.  2.  2.  2.  2.  4.  2.  2.  2.  2.  2.  2.  2.  2.  2.\n",
      "  2.  2.  2.  2.  2.  2.  4.  2.  2.  4.  2.  2.  2.  2.  2.  2.  2.  2.\n",
      "  2.  2.  2.  2.  2.  2.  2.  4.  2.  2.  4.  4.  4.  4.  2.  2.  4.  2.\n",
      "  2.  2.  2.  2.  2.  4.  4.  2.  2.  2.  4.  2.  4.  2.  4.  4.  4.  2.\n",
      "  4.  2.  2.  2.  2.  2.  2.  2.  2.  4.  4.  4.  2.  2.  4.  2.  4.  4.\n",
      "  4.  2.  2.  2.  2.  2.  2.  2.  2.  2.  2.  2.  2.  2.  4.  2.  2.  2.\n",
      "  2.  2.  2.  4.  2.  2.  4.  2.  2.  2.  2.  2.  2.  2.  2.  2.  2.  2.\n",
      "  4.  2.  2.  2.  2.  2.  2.  2.  2.  2.  4.  2.  2.  2.  2.  2.  2.  2.\n",
      "  2.  2.  4.  4.  4.  2.  2.  2.  2.  2.  2.  2.  2.  2.  4.  4.  2.  2.\n",
      "  2.  2.  2.  2.  2.  2.  2.  4.  2.  2.  2.  2.  4.  4.  4.]\n",
      "Cancer  2.0 n= 458\n",
      "Cancer  4.0 n= 241\n",
      "(699,) [ 1.  1.  1.  1.  1. -1.  1.  1.  1.  1.  1.  1. -1.  1. -1. -1.  1.  1.\n",
      " -1.  1. -1. -1.  1. -1.  1. -1.  1.  1.  1.  1.  1.  1. -1.  1.  1.  1.\n",
      " -1.  1. -1. -1.  1. -1. -1. -1. -1.  1. -1.  1.  1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1.  1. -1. -1.  1. -1.  1. -1. -1.  1.  1. -1.\n",
      "  1. -1. -1.  1.  1.  1.  1.  1.  1.  1.  1.  1. -1. -1. -1. -1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1. -1. -1. -1. -1.  1. -1. -1. -1. -1. -1.\n",
      "  1. -1.  1. -1. -1. -1.  1.  1.  1. -1.  1.  1.  1.  1. -1. -1. -1.  1.\n",
      " -1.  1. -1.  1.  1.  1. -1.  1.  1.  1.  1.  1.  1.  1.  1.  1. -1.  1.\n",
      "  1.  1. -1.  1.  1. -1.  1. -1. -1.  1.  1. -1.  1.  1.  1. -1. -1.  1.\n",
      "  1.  1.  1.  1. -1. -1.  1.  1.  1.  1.  1. -1. -1. -1.  1. -1.  1. -1.\n",
      "  1.  1.  1. -1. -1.  1. -1. -1. -1.  1. -1. -1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1. -1. -1.  1.  1.  1. -1. -1.  1.  1.  1. -1. -1.  1. -1. -1. -1.\n",
      "  1.  1. -1.  1.  1. -1. -1. -1. -1.  1. -1. -1.  1. -1. -1. -1.  1. -1.\n",
      "  1.  1. -1. -1. -1. -1.  1.  1.  1.  1.  1.  1. -1. -1.  1.  1.  1. -1.\n",
      "  1. -1. -1. -1.  1.  1.  1.  1. -1. -1. -1. -1. -1.  1. -1. -1. -1.  1.\n",
      " -1.  1. -1. -1.  1.  1.  1.  1.  1. -1.  1.  1. -1. -1. -1. -1. -1.  1.\n",
      " -1. -1.  1.  1. -1. -1.  1. -1.  1.  1.  1. -1. -1.  1. -1.  1. -1. -1.\n",
      "  1.  1. -1.  1.  1.  1. -1.  1.  1.  1. -1. -1.  1.  1. -1.  1.  1. -1.\n",
      "  1.  1. -1.  1. -1. -1. -1.  1.  1. -1. -1.  1. -1.  1.  1. -1. -1.  1.\n",
      "  1.  1. -1.  1.  1.  1. -1. -1.  1.  1.  1. -1.  1.  1. -1. -1. -1. -1.\n",
      " -1. -1.  1.  1.  1.  1. -1. -1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1. -1.  1.  1.  1.  1. -1.  1.  1.  1.  1. -1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1. -1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1. -1.  1.\n",
      " -1.  1. -1.  1.  1.  1.  1. -1.  1.  1.  1. -1.  1. -1.  1.  1.  1.  1.\n",
      "  1.  1.  1. -1. -1.  1.  1.  1. -1.  1.  1.  1.  1.  1.  1.  1.  1. -1.\n",
      "  1.  1.  1. -1.  1. -1. -1. -1.  1.  1.  1.  1.  1.  1.  1. -1. -1. -1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1. -1.  1.  1. -1. -1.  1.  1.\n",
      "  1. -1. -1. -1.  1. -1.  1. -1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1. -1.  1.  1.  1.  1.  1.  1.  1. -1. -1.  1.  1.  1. -1.  1.  1.\n",
      " -1. -1.  1.  1.  1.  1.  1.  1. -1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1. -1.  1.  1. -1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1. -1.  1.  1. -1. -1. -1. -1.  1.  1. -1.  1.\n",
      "  1.  1.  1.  1.  1. -1. -1.  1.  1.  1. -1.  1. -1.  1. -1. -1. -1.  1.\n",
      " -1.  1.  1.  1.  1.  1.  1.  1.  1. -1. -1. -1.  1.  1. -1.  1. -1. -1.\n",
      " -1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1. -1.  1.  1.  1.\n",
      "  1.  1.  1. -1.  1.  1. -1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      " -1.  1.  1.  1.  1.  1.  1.  1.  1.  1. -1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1. -1. -1. -1.  1.  1.  1.  1.  1.  1.  1.  1.  1. -1. -1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1. -1.  1.  1.  1.  1. -1. -1. -1.]\n",
      "--\n",
      "[  1.07170410e+06   4.41773963e+00   3.13447783e+00   3.20743920e+00\n",
      "   2.80686695e+00   3.21602289e+00   3.48640916e+00   3.43776824e+00\n",
      "   2.86695279e+00   1.58941345e+00]\n",
      "[  6.16654159e+05   2.81372582e+00   3.04927560e+00   2.96978617e+00\n",
      "   2.85333603e+00   2.21271541e+00   3.61933705e+00   2.43661945e+00\n",
      "   3.05144882e+00   1.71385070e+00]\n",
      "min datos [  6.16340000e+04   1.00000000e+00   1.00000000e+00   1.00000000e+00\n",
      "   1.00000000e+00   1.00000000e+00   1.00000000e+00   1.00000000e+00\n",
      "   1.00000000e+00   1.00000000e+00]\n",
      "max datos [  1.34543520e+07   1.00000000e+01   1.00000000e+01   1.00000000e+01\n",
      "   1.00000000e+01   1.00000000e+01   1.00000000e+01   1.00000000e+01\n",
      "   1.00000000e+01   1.00000000e+01]\n",
      "--\n",
      "min max datos columns\n",
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.] [ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]\n"
     ]
    }
   ],
   "source": [
    "cancer = np.loadtxt(\"data/datasetCancer.csv\", delimiter=\";\")\n",
    "datos = cancer[:,:10]\n",
    "clase = cancer[:,10]\n",
    "print(cancer.shape)\n",
    "print(datos.shape,datos)\n",
    "print(clase.shape,clase)\n",
    "\n",
    "for tipoCancer in np.unique(clase):\n",
    "    print (\"Cancer \", tipoCancer, \"n=\", np.sum(clase==tipoCancer))\n",
    "clase[clase==2]=1\n",
    "clase[clase==4]=-1\n",
    "print (\"--\")\n",
    "print (np.mean(datos, axis=0) )   \n",
    "print (np.std(datos, axis=0)  )  \n",
    "print (\"min datos\", np.min(datos, axis=0))\n",
    "print (\"max datos\", np.max(datos, axis=0))\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import scale, MinMaxScaler \n",
    "\n",
    "print (\"--\")\n",
    "datos = MinMaxScaler().fit_transform(datos)\n",
    "print (\"min max datos columns\")\n",
    "print (np.min(datos, axis=0), np.max(datos, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5; Error: 16.336438928238714;\n",
      "Epoch: 10; Error: 9.283869972719568;\n",
      "Epoch: 15; Error: 6.648954989587514;\n",
      "Epoch: 20; Error: 2.9300563015825762;\n",
      "Epoch: 25; Error: 2.0506014729908895;\n",
      "Epoch: 30; Error: 2.0088762914412164;\n",
      "Epoch: 35; Error: 2.001539030495024;\n",
      "Epoch: 40; Error: 1.9997117247902139;\n",
      "Epoch: 45; Error: 1.9995896890755658;\n",
      "Epoch: 50; Error: 1.9993304230030637;\n",
      "Epoch: 55; Error: 1.9903098618804576;\n",
      "Epoch: 60; Error: 1.9340581224124784;\n",
      "Epoch: 65; Error: 1.884781203206434;\n",
      "Epoch: 70; Error: 1.8652521732341236;\n",
      "Epoch: 75; Error: 1.7215938562528046;\n",
      "Epoch: 80; Error: 1.6000178126410332;\n",
      "Epoch: 85; Error: 1.4240283507302587;\n",
      "Epoch: 90; Error: 1.1509577465259033;\n",
      "Epoch: 95; Error: 0.15986509358309312;\n",
      "The goal of learning is reached\n"
     ]
    }
   ],
   "source": [
    "# Definimos y entrenamos una red con dos capas intermedias y una neurona de salida\n",
    "from sklearn import cross_validation\n",
    "import neurolab as nl\n",
    "\n",
    "limits = np.vstack((np.min(datos)*np.ones(datos.shape[1]), np.max(datos)*np.ones(datos.shape[1]))).T\n",
    "test_size = 0.6\n",
    "\n",
    "datos_train, datos_test, clase_train, clase_test = cross_validation.train_test_split(datos, \n",
    "                                                                     clase, \n",
    "                                                                     test_size=test_size, \n",
    "                                                                     random_state=np.random.randint(1,100))\n",
    "net = nl.net.newff(limits,[6, 1])\n",
    "err = net.train(datos_train, clase_train.reshape(len(clase_train), 1), show=5, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0,0.5,'error (default SSE)')"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEKCAYAAAAIO8L1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nq+m/AX0tv5DYA/I/AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x234bf286160>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(err)\n",
    "plt.xlabel('Epoch number')\n",
    "plt.ylabel('error (default SSE)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(420, 10)\n",
      "[[143   0]\n",
      " [277   0]]\n",
      "desempeño en el dataset de TEST:  0.340476190476\n"
     ]
    }
   ],
   "source": [
    "# miramos el desempeño en el dataset de TEST\n",
    "from sklearn.metrics import confusion_matrix\n",
    "print(datos_test.shape)\n",
    "pred = net.sim(datos_test)[:,0]\n",
    "pred[pred<5]=-1\n",
    "pred[pred>5]=1\n",
    "\n",
    "cm = confusion_matrix(clase_test, pred)\n",
    "print (cm)\n",
    "print (\"desempeño en el dataset de TEST: \",np.sum(np.diag(cm))*1./np.sum(cm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 98   0]\n",
      " [  0 181]]\n",
      "desempeño en el dataset de TRAIN:  1.0\n"
     ]
    }
   ],
   "source": [
    "# miramos el desempeño en el dataset de TRAIN\n",
    "pred =  net.sim(datos_train)[:,0]\n",
    "pred[pred<0]=-1\n",
    "pred[pred>0]=1\n",
    "\n",
    "cm = confusion_matrix(clase_train, pred)\n",
    "print (cm)\n",
    "print (\"desempeño en el dataset de TRAIN: \",np.sum(np.diag(cm))*1./np.sum(cm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acierto en test  0.957142857143\n",
      "[[139   4]\n",
      " [ 14 263]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "g = GaussianNB()\n",
    "g.fit(datos_train, clase_train)\n",
    "predicciones_test  = g.predict(datos_test)\n",
    "cm=confusion_matrix(clase_test, predicciones_test)\n",
    "print (\"acierto en test \", g.score(datos_test, clase_test))\n",
    "#print (cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pruebas con varios de los clasificadores de SciKit-Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for DecisionTree: 91.42857142857143\n",
      "Accuracy for SVM: 95.95238095238095\n",
      "Accuracy for perceptron: 96.9047619047619\n",
      "Accuracy for KNN: 95.71428571428572\n",
      "Accuracy for GNB: 95.71428571428572\n",
      "Best gender classifier is Perceptron\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Juan Pulido\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.perceptron.Perceptron'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn import tree\n",
    "from sklearn.svm import SVC # Supported Vector Classificator\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "import numpy as np\n",
    "\n",
    "#CLASSIFIERS\n",
    "# using the default values for all the hyperparameters\n",
    "clf_tree = tree.DecisionTreeClassifier()\n",
    "clf_SVM = SVC()\n",
    "clf_Perceptron = Perceptron()\n",
    "clf_KNN = KNeighborsClassifier()\n",
    "clf_GNB = GaussianNB()\n",
    "\n",
    "# Training the models\n",
    "clf_tree = clf_tree.fit(datos_train, clase_train)\n",
    "clf_SVM.fit(datos_train, clase_train)\n",
    "clf_Perceptron.fit(datos_train, clase_train)\n",
    "clf_KNN.fit(datos_train, clase_train)\n",
    "clf_GNB.fit(datos_train, clase_train)\n",
    "\n",
    "# Testing using the same data\n",
    "pred_tree = clf_tree.predict(datos_test)\n",
    "acc_tree = accuracy_score(clase_test, pred_tree) * 100\n",
    "print('Accuracy for DecisionTree: {}'.format(acc_tree))\n",
    "\n",
    "pred_svm = clf_SVM.predict(datos_test)\n",
    "acc_svm = accuracy_score(clase_test, pred_svm) * 100\n",
    "print('Accuracy for SVM: {}'.format(acc_svm))\n",
    "\n",
    "pred_per = clf_Perceptron.predict(datos_test)\n",
    "acc_per = accuracy_score(clase_test, pred_per) * 100\n",
    "print('Accuracy for perceptron: {}'.format(acc_per))\n",
    "\n",
    "pred_KNN = clf_KNN.predict(datos_test)\n",
    "acc_KNN = accuracy_score(clase_test, pred_KNN) * 100\n",
    "print('Accuracy for KNN: {}'.format(acc_KNN))\n",
    "\n",
    "pred_GNB = clf_GNB.predict(datos_test)\n",
    "acc_GNB = accuracy_score(clase_test, pred_GNB) * 100\n",
    "print('Accuracy for GNB: {}'.format(acc_GNB))\n",
    "\n",
    "# The best classifier from svm, per, KNN\n",
    "index = np.argmax([acc_svm, acc_per, acc_KNN, acc_GNB])\n",
    "classifiers = {0: 'SVM', 1: 'Perceptron', 2: 'KNN', 3: 'GNB'}\n",
    "print('Best gender classifier is {}'.format(classifiers[index]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
